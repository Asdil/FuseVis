{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the pytorch implementation of the deep learning model proposed in the paper 'DenseFuse: A Fusion Approach to Infrared and Visible Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models.vgg import vgg19\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from skimage import img_as_ubyte\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision      # dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import argparse\n",
    "import glob\n",
    "import imageio\n",
    "from skimage import color\n",
    "import numpy\n",
    "import natsort\n",
    "import scipy\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "import pprint\n",
    "from scipy.ndimage import correlate\n",
    "from scipy.ndimage.filters import gaussian_gradient_magnitude\n",
    "import torchvision.datasets as dset\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import os.path\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "import tkinter.font as tkFont\n",
    "from PIL import ImageTk, Image\n",
    "import pylab\n",
    "import cv2\n",
    "import h5py\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11811160064\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.cuda.get_device_properties(0).total_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the hyperparameters\n",
    "image_length = 256\n",
    "image_width  = 256\n",
    "mr_channels  = 1\n",
    "gray_channels = 1\n",
    "pet_channels = 4    \n",
    "rgb_channels = 3     \n",
    "batch_size   = 1\n",
    "EPOCH = 50\n",
    "learning_rate = 0.02 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the train mri data\n",
    "filenames = os.listdir('C:/Users/horan/Desktop/FuseVis/Training/MRI')\n",
    "dataset = os.path.join(os.getcwd(), 'C:/Users/horan/Desktop/FuseVis/Training/MRI')\n",
    "data = glob.glob(os.path.join(dataset, \"*.gif\"))\n",
    "data = natsort.natsorted(data,reverse=False)\n",
    "train_mri = np.zeros((len(data), image_width,image_length))\n",
    "for i in range(len(data)):\n",
    "    train_mri[i,:,:] =(imageio.imread(data[i]))\n",
    "    train_mri[i,:,:] =(train_mri[i,:,:] - np.min(train_mri[i,:,:])) / (np.max(train_mri[i,:,:]) - np.min(train_mri[i,:,:]))\n",
    "    train_mri[i,:,:] = np.float32(train_mri[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand dimension to add the channel\n",
    "train_mri = np.expand_dims(train_mri,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272, 1, 256, 256)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify the shape matches the pytorch standard\n",
    "train_mri.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([272, 1, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the MRI training data to pytorch tensor\n",
    "train_mri_tensor = torch.from_numpy(train_mri).float()\n",
    "train_mri_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the train pet data\n",
    "filenames = os.listdir('C:/Users/horan/Desktop/FuseVis/Training/PET')\n",
    "dataset = os.path.join(os.getcwd(), 'C:/Users/horan/Desktop/FuseVis/Training/PET')\n",
    "data = glob.glob(os.path.join(dataset, \"*.gif\"))\n",
    "data = natsort.natsorted(data,reverse=False)\n",
    "train_other = np.zeros((len(data),image_width,image_length,pet_channels),dtype=float)\n",
    "train_pet = np.zeros((len(data),image_width,image_length),dtype=float)\n",
    "for i in range(len(data)):\n",
    "    train_other[i,:,:,:] =(imageio.imread(data[i]))\n",
    "    train_pet[i,:,:] = 0.2989 * train_other[i,:,:,0] + 0.5870 *  train_other[i,:,:,1]  + 0.1140 * train_other[i,:,:,2]\n",
    "    train_pet[i,:,:] =(train_pet[i,:,:] - np.min(train_pet[i,:,:])) / (np.max(train_pet[i,:,:]) - np.min(train_pet[i,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand the dimension to add the channel\n",
    "train_pet = np.expand_dims(train_pet,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272, 1, 256, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify the shape matches the pytorch standard\n",
    "train_pet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([272, 1, 256, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the PET training data to pytorch tensor\n",
    "train_pet_tensor = torch.from_numpy(train_pet).float()\n",
    "train_pet_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseFuse_net(\n",
      "  (conv1): ConvLayer(\n",
      "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv2d): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (dropout): Dropout2d(p=0.5)\n",
      "  )\n",
      "  (DB1): DenseBlock(\n",
      "    (denseblock): Sequential(\n",
      "      (0): DenseConv2d(\n",
      "        (dense_conv): ConvLayer(\n",
      "          (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "          (conv2d): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (dropout): Dropout2d(p=0.5)\n",
      "        )\n",
      "      )\n",
      "      (1): DenseConv2d(\n",
      "        (dense_conv): ConvLayer(\n",
      "          (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "          (conv2d): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (dropout): Dropout2d(p=0.5)\n",
      "        )\n",
      "      )\n",
      "      (2): DenseConv2d(\n",
      "        (dense_conv): ConvLayer(\n",
      "          (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "          (conv2d): Conv2d(48, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (dropout): Dropout2d(p=0.5)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv2): ConvLayer(\n",
      "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (dropout): Dropout2d(p=0.5)\n",
      "  )\n",
      "  (conv3): ConvLayer(\n",
      "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv2d): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (dropout): Dropout2d(p=0.5)\n",
      "  )\n",
      "  (conv4): ConvLayer(\n",
      "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv2d): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (dropout): Dropout2d(p=0.5)\n",
      "  )\n",
      "  (conv5): ConvLayer(\n",
      "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv2d): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (dropout): Dropout2d(p=0.5)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#define the fusion network\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, is_last=False):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = int(np.floor(kernel_size / 2))\n",
    "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "        self.is_last = is_last\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        if self.is_last is False:\n",
    "            out = F.relu(out, inplace=True)\n",
    "        return out\n",
    "\n",
    "# Dense convolution unit\n",
    "class DenseConv2d(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(DenseConv2d, self).__init__()\n",
    "        self.dense_conv = ConvLayer(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dense_conv(x)\n",
    "        out = torch.cat([x, out], 1)\n",
    "        return out\n",
    "\n",
    "# Dense Block unit\n",
    "class DenseBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, stride):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        out_channels_def = 16\n",
    "        denseblock = []\n",
    "        denseblock += [DenseConv2d(in_channels, out_channels_def, kernel_size, stride),\n",
    "                       DenseConv2d(in_channels+out_channels_def, out_channels_def, kernel_size, stride),\n",
    "                       DenseConv2d(in_channels+out_channels_def*2, out_channels_def, kernel_size, stride)]\n",
    "        self.denseblock = nn.Sequential(*denseblock)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.denseblock(x)\n",
    "        return out\n",
    "\n",
    "# DenseFuse network\n",
    "class DenseFuse_net(nn.Module):\n",
    "    def __init__(self, input_nc=1, output_nc=1):\n",
    "        super(DenseFuse_net, self).__init__()\n",
    "        denseblock = DenseBlock\n",
    "        nb_filter = [16, 64, 32, 16]\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        # encoder\n",
    "        self.conv1 = ConvLayer(input_nc, nb_filter[0], kernel_size, stride)\n",
    "        self.DB1 = denseblock(nb_filter[0], kernel_size, stride)\n",
    "        # decoder\n",
    "        self.conv2 = ConvLayer(nb_filter[1], nb_filter[1], kernel_size, stride)\n",
    "        self.conv3 = ConvLayer(nb_filter[1], nb_filter[2], kernel_size, stride)\n",
    "        self.conv4 = ConvLayer(nb_filter[2], nb_filter[3], kernel_size, stride)\n",
    "        self.conv5 = ConvLayer(nb_filter[3], output_nc, kernel_size, stride)\n",
    "\n",
    "    def encoder(self, input):\n",
    "        x1 = self.conv1(input)\n",
    "        x_DB = self.DB1(x1)\n",
    "        return x_DB\n",
    "    \n",
    "    def fusion(self, en1, en2, strategy_type='addition'):\n",
    "        f_0 = (en1 + en2)/2\n",
    "        return f_0\n",
    "\n",
    "    def decoder(self, f_en):\n",
    "        x2 = self.conv2(f_en)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        output = self.conv5(x4)\n",
    "        return output\n",
    "    \n",
    "cnn = DenseFuse_net().to(device)\n",
    "cnn = cnn.float()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the optimizers and loss functions \n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)   #optimize all cnn parameters\n",
    "MSE_loss   = nn.MSELoss()                                           #MSEloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0],step: [100], ssim_loss_mri: [0.32261705], ssim_loss_pet: [0.04837583]\n",
      "Epoch: [ 0],step: [200], ssim_loss_mri: [0.65731442], ssim_loss_pet: [0.12659176]\n",
      "Epoch: [ 1],step: [300], ssim_loss_mri: [0.43097734], ssim_loss_pet: [0.08039657]\n",
      "Epoch: [ 1],step: [400], ssim_loss_mri: [0.25270671], ssim_loss_pet: [0.03440311]\n",
      "Epoch: [ 1],step: [500], ssim_loss_mri: [0.91602558], ssim_loss_pet: [0.08732969]\n",
      "Epoch: [ 2],step: [600], ssim_loss_mri: [0.63139117], ssim_loss_pet: [0.13419668]\n",
      "Epoch: [ 2],step: [700], ssim_loss_mri: [0.46563876], ssim_loss_pet: [0.04640470]\n",
      "Epoch: [ 2],step: [800], ssim_loss_mri: [0.39394110], ssim_loss_pet: [0.03610177]\n",
      "Epoch: [ 3],step: [900], ssim_loss_mri: [0.33492613], ssim_loss_pet: [0.06023899]\n",
      "Epoch: [ 3],step: [1000], ssim_loss_mri: [0.61049080], ssim_loss_pet: [0.09794085]\n",
      "Epoch: [ 4],step: [1100], ssim_loss_mri: [0.38081795], ssim_loss_pet: [0.06649068]\n",
      "Epoch: [ 4],step: [1200], ssim_loss_mri: [0.29977316], ssim_loss_pet: [0.04245850]\n",
      "Epoch: [ 4],step: [1300], ssim_loss_mri: [0.40956986], ssim_loss_pet: [0.04487916]\n",
      "Epoch: [ 5],step: [1400], ssim_loss_mri: [0.53737998], ssim_loss_pet: [0.08184113]\n",
      "Epoch: [ 5],step: [1500], ssim_loss_mri: [0.17516679], ssim_loss_pet: [0.03059424]\n",
      "Epoch: [ 5],step: [1600], ssim_loss_mri: [0.52110279], ssim_loss_pet: [0.06407793]\n",
      "Epoch: [ 6],step: [1700], ssim_loss_mri: [0.61655581], ssim_loss_pet: [0.17007764]\n",
      "Epoch: [ 6],step: [1800], ssim_loss_mri: [0.47024298], ssim_loss_pet: [0.08885521]\n",
      "Epoch: [ 6],step: [1900], ssim_loss_mri: [0.45831132], ssim_loss_pet: [0.07341862]\n",
      "Epoch: [ 7],step: [2000], ssim_loss_mri: [0.32915026], ssim_loss_pet: [0.05125855]\n",
      "Epoch: [ 7],step: [2100], ssim_loss_mri: [0.63501441], ssim_loss_pet: [0.09092657]\n",
      "Epoch: [ 8],step: [2200], ssim_loss_mri: [0.52464187], ssim_loss_pet: [0.09188315]\n",
      "Epoch: [ 8],step: [2300], ssim_loss_mri: [0.26924074], ssim_loss_pet: [0.03520691]\n",
      "Epoch: [ 8],step: [2400], ssim_loss_mri: [0.90175909], ssim_loss_pet: [0.13024978]\n",
      "Epoch: [ 9],step: [2500], ssim_loss_mri: [0.36191529], ssim_loss_pet: [0.05650842]\n",
      "Epoch: [ 9],step: [2600], ssim_loss_mri: [0.26816750], ssim_loss_pet: [0.05541096]\n",
      "Epoch: [ 9],step: [2700], ssim_loss_mri: [0.48830336], ssim_loss_pet: [0.08792447]\n",
      "Epoch: [10],step: [2800], ssim_loss_mri: [0.33301467], ssim_loss_pet: [0.07262339]\n",
      "Epoch: [10],step: [2900], ssim_loss_mri: [0.57542169], ssim_loss_pet: [0.09217878]\n",
      "Epoch: [11],step: [3000], ssim_loss_mri: [0.48071778], ssim_loss_pet: [0.08624109]\n",
      "Epoch: [11],step: [3100], ssim_loss_mri: [0.30932873], ssim_loss_pet: [0.04556768]\n",
      "Epoch: [11],step: [3200], ssim_loss_mri: [0.55193913], ssim_loss_pet: [0.08003557]\n",
      "Epoch: [12],step: [3300], ssim_loss_mri: [0.55328894], ssim_loss_pet: [0.08334507]\n",
      "Epoch: [12],step: [3400], ssim_loss_mri: [0.20387185], ssim_loss_pet: [0.03171767]\n",
      "Epoch: [12],step: [3500], ssim_loss_mri: [0.56633389], ssim_loss_pet: [0.06925593]\n",
      "Epoch: [13],step: [3600], ssim_loss_mri: [0.62886864], ssim_loss_pet: [0.16664833]\n",
      "Epoch: [13],step: [3700], ssim_loss_mri: [0.48080891], ssim_loss_pet: [0.06869095]\n",
      "Epoch: [13],step: [3800], ssim_loss_mri: [0.57716346], ssim_loss_pet: [0.11299261]\n",
      "Epoch: [14],step: [3900], ssim_loss_mri: [0.33372277], ssim_loss_pet: [0.05633624]\n",
      "Epoch: [14],step: [4000], ssim_loss_mri: [0.49675614], ssim_loss_pet: [0.07129884]\n",
      "Epoch: [15],step: [4100], ssim_loss_mri: [0.56750667], ssim_loss_pet: [0.09941376]\n",
      "Epoch: [15],step: [4200], ssim_loss_mri: [0.28260261], ssim_loss_pet: [0.03964719]\n",
      "Epoch: [15],step: [4300], ssim_loss_mri: [0.91568309], ssim_loss_pet: [0.08752041]\n",
      "Epoch: [16],step: [4400], ssim_loss_mri: [0.43796587], ssim_loss_pet: [0.06752180]\n",
      "Epoch: [16],step: [4500], ssim_loss_mri: [0.47164148], ssim_loss_pet: [0.07967526]\n",
      "Epoch: [16],step: [4600], ssim_loss_mri: [0.50380385], ssim_loss_pet: [0.09155869]\n",
      "Epoch: [17],step: [4700], ssim_loss_mri: [0.50541425], ssim_loss_pet: [0.15911737]\n",
      "Epoch: [17],step: [4800], ssim_loss_mri: [0.41200691], ssim_loss_pet: [0.08513597]\n",
      "Epoch: [18],step: [4900], ssim_loss_mri: [0.49559903], ssim_loss_pet: [0.10055463]\n",
      "Epoch: [18],step: [5000], ssim_loss_mri: [0.31705022], ssim_loss_pet: [0.04753479]\n",
      "Epoch: [18],step: [5100], ssim_loss_mri: [0.62354243], ssim_loss_pet: [0.09641942]\n",
      "Epoch: [19],step: [5200], ssim_loss_mri: [0.55938232], ssim_loss_pet: [0.08324850]\n",
      "Epoch: [19],step: [5300], ssim_loss_mri: [0.23188013], ssim_loss_pet: [0.03405333]\n",
      "Epoch: [19],step: [5400], ssim_loss_mri: [0.57432574], ssim_loss_pet: [0.07899171]\n",
      "Epoch: [20],step: [5500], ssim_loss_mri: [0.64560294], ssim_loss_pet: [0.17069757]\n",
      "Epoch: [20],step: [5600], ssim_loss_mri: [0.48512113], ssim_loss_pet: [0.05155934]\n",
      "Epoch: [20],step: [5700], ssim_loss_mri: [0.18456376], ssim_loss_pet: [0.01160559]\n",
      "Epoch: [21],step: [5800], ssim_loss_mri: [0.33541340], ssim_loss_pet: [0.05876838]\n",
      "Epoch: [21],step: [5900], ssim_loss_mri: [0.59120798], ssim_loss_pet: [0.11381344]\n",
      "Epoch: [22],step: [6000], ssim_loss_mri: [0.55957466], ssim_loss_pet: [0.07541528]\n",
      "Epoch: [22],step: [6100], ssim_loss_mri: [0.29422414], ssim_loss_pet: [0.04163098]\n",
      "Epoch: [22],step: [6200], ssim_loss_mri: [0.92954987], ssim_loss_pet: [0.07119004]\n",
      "Epoch: [23],step: [6300], ssim_loss_mri: [0.49265939], ssim_loss_pet: [0.07375039]\n",
      "Epoch: [23],step: [6400], ssim_loss_mri: [0.56633008], ssim_loss_pet: [0.10744812]\n",
      "Epoch: [23],step: [6500], ssim_loss_mri: [0.42823607], ssim_loss_pet: [0.06077555]\n",
      "Epoch: [24],step: [6600], ssim_loss_mri: [0.57673442], ssim_loss_pet: [0.16791131]\n",
      "Epoch: [24],step: [6700], ssim_loss_mri: [0.44814277], ssim_loss_pet: [0.09974489]\n",
      "Epoch: [24],step: [6800], ssim_loss_mri: [0.61160827], ssim_loss_pet: [0.10186370]\n",
      "Epoch: [25],step: [6900], ssim_loss_mri: [0.32261705], ssim_loss_pet: [0.04837583]\n",
      "Epoch: [25],step: [7000], ssim_loss_mri: [0.65731442], ssim_loss_pet: [0.12659176]\n",
      "Epoch: [26],step: [7100], ssim_loss_mri: [0.43097734], ssim_loss_pet: [0.08039657]\n",
      "Epoch: [26],step: [7200], ssim_loss_mri: [0.25270671], ssim_loss_pet: [0.03440311]\n",
      "Epoch: [26],step: [7300], ssim_loss_mri: [0.91602558], ssim_loss_pet: [0.08732969]\n",
      "Epoch: [27],step: [7400], ssim_loss_mri: [0.63139117], ssim_loss_pet: [0.13419668]\n",
      "Epoch: [27],step: [7500], ssim_loss_mri: [0.46563876], ssim_loss_pet: [0.04640470]\n",
      "Epoch: [27],step: [7600], ssim_loss_mri: [0.39394110], ssim_loss_pet: [0.03610177]\n",
      "Epoch: [28],step: [7700], ssim_loss_mri: [0.33492613], ssim_loss_pet: [0.06023899]\n",
      "Epoch: [28],step: [7800], ssim_loss_mri: [0.61049080], ssim_loss_pet: [0.09794085]\n",
      "Epoch: [29],step: [7900], ssim_loss_mri: [0.38081795], ssim_loss_pet: [0.06649068]\n",
      "Epoch: [29],step: [8000], ssim_loss_mri: [0.29977316], ssim_loss_pet: [0.04245850]\n",
      "Epoch: [29],step: [8100], ssim_loss_mri: [0.40956986], ssim_loss_pet: [0.04487916]\n",
      "Epoch: [30],step: [8200], ssim_loss_mri: [0.53737998], ssim_loss_pet: [0.08184113]\n",
      "Epoch: [30],step: [8300], ssim_loss_mri: [0.17516679], ssim_loss_pet: [0.03059424]\n",
      "Epoch: [30],step: [8400], ssim_loss_mri: [0.52110279], ssim_loss_pet: [0.06407793]\n",
      "Epoch: [31],step: [8500], ssim_loss_mri: [0.61655581], ssim_loss_pet: [0.17007764]\n",
      "Epoch: [31],step: [8600], ssim_loss_mri: [0.47024298], ssim_loss_pet: [0.08885521]\n",
      "Epoch: [31],step: [8700], ssim_loss_mri: [0.45831132], ssim_loss_pet: [0.07341862]\n",
      "Epoch: [32],step: [8800], ssim_loss_mri: [0.32915026], ssim_loss_pet: [0.05125855]\n",
      "Epoch: [32],step: [8900], ssim_loss_mri: [0.63501441], ssim_loss_pet: [0.09092657]\n",
      "Epoch: [33],step: [9000], ssim_loss_mri: [0.52464187], ssim_loss_pet: [0.09188315]\n",
      "Epoch: [33],step: [9100], ssim_loss_mri: [0.26924074], ssim_loss_pet: [0.03520691]\n",
      "Epoch: [33],step: [9200], ssim_loss_mri: [0.90175909], ssim_loss_pet: [0.13024978]\n",
      "Epoch: [34],step: [9300], ssim_loss_mri: [0.36191529], ssim_loss_pet: [0.05650842]\n",
      "Epoch: [34],step: [9400], ssim_loss_mri: [0.26816750], ssim_loss_pet: [0.05541096]\n",
      "Epoch: [34],step: [9500], ssim_loss_mri: [0.48830336], ssim_loss_pet: [0.08792447]\n",
      "Epoch: [35],step: [9600], ssim_loss_mri: [0.33301467], ssim_loss_pet: [0.07262339]\n",
      "Epoch: [35],step: [9700], ssim_loss_mri: [0.57542169], ssim_loss_pet: [0.09217878]\n",
      "Epoch: [36],step: [9800], ssim_loss_mri: [0.48071778], ssim_loss_pet: [0.08624109]\n",
      "Epoch: [36],step: [9900], ssim_loss_mri: [0.30932873], ssim_loss_pet: [0.04556768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36],step: [10000], ssim_loss_mri: [0.55193913], ssim_loss_pet: [0.08003557]\n",
      "Epoch: [37],step: [10100], ssim_loss_mri: [0.55328894], ssim_loss_pet: [0.08334507]\n",
      "Epoch: [37],step: [10200], ssim_loss_mri: [0.20387185], ssim_loss_pet: [0.03171767]\n",
      "Epoch: [37],step: [10300], ssim_loss_mri: [0.56633389], ssim_loss_pet: [0.06925593]\n",
      "Epoch: [38],step: [10400], ssim_loss_mri: [0.62886864], ssim_loss_pet: [0.16664833]\n",
      "Epoch: [38],step: [10500], ssim_loss_mri: [0.48080891], ssim_loss_pet: [0.06869095]\n",
      "Epoch: [38],step: [10600], ssim_loss_mri: [0.57716346], ssim_loss_pet: [0.11299261]\n",
      "Epoch: [39],step: [10700], ssim_loss_mri: [0.33372277], ssim_loss_pet: [0.05633624]\n",
      "Epoch: [39],step: [10800], ssim_loss_mri: [0.49675614], ssim_loss_pet: [0.07129884]\n",
      "Epoch: [40],step: [10900], ssim_loss_mri: [0.56750667], ssim_loss_pet: [0.09941376]\n",
      "Epoch: [40],step: [11000], ssim_loss_mri: [0.28260261], ssim_loss_pet: [0.03964719]\n",
      "Epoch: [40],step: [11100], ssim_loss_mri: [0.91568309], ssim_loss_pet: [0.08752041]\n",
      "Epoch: [41],step: [11200], ssim_loss_mri: [0.43796587], ssim_loss_pet: [0.06752180]\n",
      "Epoch: [41],step: [11300], ssim_loss_mri: [0.47164148], ssim_loss_pet: [0.07967526]\n",
      "Epoch: [41],step: [11400], ssim_loss_mri: [0.50380385], ssim_loss_pet: [0.09155869]\n",
      "Epoch: [42],step: [11500], ssim_loss_mri: [0.50541425], ssim_loss_pet: [0.15911737]\n",
      "Epoch: [42],step: [11600], ssim_loss_mri: [0.41200691], ssim_loss_pet: [0.08513597]\n",
      "Epoch: [43],step: [11700], ssim_loss_mri: [0.49559903], ssim_loss_pet: [0.10055463]\n",
      "Epoch: [43],step: [11800], ssim_loss_mri: [0.31705022], ssim_loss_pet: [0.04753479]\n",
      "Epoch: [43],step: [11900], ssim_loss_mri: [0.62354243], ssim_loss_pet: [0.09641942]\n",
      "Epoch: [44],step: [12000], ssim_loss_mri: [0.55938232], ssim_loss_pet: [0.08324850]\n",
      "Epoch: [44],step: [12100], ssim_loss_mri: [0.23188013], ssim_loss_pet: [0.03405333]\n",
      "Epoch: [44],step: [12200], ssim_loss_mri: [0.57432574], ssim_loss_pet: [0.07899171]\n",
      "Epoch: [45],step: [12300], ssim_loss_mri: [0.64560294], ssim_loss_pet: [0.17069757]\n",
      "Epoch: [45],step: [12400], ssim_loss_mri: [0.48512113], ssim_loss_pet: [0.05155934]\n",
      "Epoch: [45],step: [12500], ssim_loss_mri: [0.18456376], ssim_loss_pet: [0.01160559]\n",
      "Epoch: [46],step: [12600], ssim_loss_mri: [0.33541340], ssim_loss_pet: [0.05876838]\n",
      "Epoch: [46],step: [12700], ssim_loss_mri: [0.59120798], ssim_loss_pet: [0.11381344]\n",
      "Epoch: [47],step: [12800], ssim_loss_mri: [0.55957466], ssim_loss_pet: [0.07541528]\n",
      "Epoch: [47],step: [12900], ssim_loss_mri: [0.29422414], ssim_loss_pet: [0.04163098]\n",
      "Epoch: [47],step: [13000], ssim_loss_mri: [0.92954987], ssim_loss_pet: [0.07119004]\n",
      "Epoch: [48],step: [13100], ssim_loss_mri: [0.49265939], ssim_loss_pet: [0.07375039]\n",
      "Epoch: [48],step: [13200], ssim_loss_mri: [0.56633008], ssim_loss_pet: [0.10744812]\n",
      "Epoch: [48],step: [13300], ssim_loss_mri: [0.42823607], ssim_loss_pet: [0.06077555]\n",
      "Epoch: [49],step: [13400], ssim_loss_mri: [0.57673442], ssim_loss_pet: [0.16791131]\n",
      "Epoch: [49],step: [13500], ssim_loss_mri: [0.44814277], ssim_loss_pet: [0.09974489]\n",
      "Epoch: [49],step: [13600], ssim_loss_mri: [0.61160827], ssim_loss_pet: [0.10186370]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type DenseFuse_net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type ConvLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type DenseBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type DenseConv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# perform the training\n",
    "counter = 0\n",
    "start_time = time.time()\n",
    "lamda = 1\n",
    "ep_ssim_loss_mri = []\n",
    "ep_l2_loss_mri = []\n",
    "for epoch in range(EPOCH):\n",
    "    ssim_Loss_mri = []\n",
    "    l2_Loss_mri = []\n",
    "    #run batch images\n",
    "    batch_idxs = 272 // batch_size\n",
    "    for idx in range(0, batch_idxs):\n",
    "        #reconstruct mri\n",
    "        b_x = train_mri_tensor[idx*batch_size : (idx+1)*batch_size,:,:,:].to(device)\n",
    "        counter += 1\n",
    "        #encoder\n",
    "        en = cnn.encoder(b_x)              \n",
    "        #decoder\n",
    "        output = cnn.decoder(en)\n",
    "        ssim_loss_mri = 1 - ssim(output, b_x,data_range=1)\n",
    "        l2_loss_mri = MSE_loss(output,b_x) \n",
    "        loss_total_mri = lamda*ssim_loss_mri + (1-lamda)*l2_loss_mri\n",
    "        optimizer.zero_grad()              # clear gradients for this training step\n",
    "        loss_total_mri.backward()              # backpropagation, compute gradients\n",
    "        optimizer.step()                   # apply gradients\n",
    "\n",
    "        ssim_Loss_mri.append(ssim_loss_mri.item())\n",
    "        l2_Loss_mri.append(l2_loss_mri.item())\n",
    "        \n",
    "        if counter % 100 == 0:\n",
    "            print(\"Epoch: [%2d],step: [%2d], ssim_loss_mri: [%.8f], ssim_loss_pet: [%.8f]\" \n",
    "                   %(epoch, counter, ssim_loss_mri, l2_loss_mri))\n",
    "    \n",
    "    av_ssim_loss_mri = np.average(ssim_Loss_mri)\n",
    "    ep_ssim_loss_mri.append(av_ssim_loss_mri)\n",
    "    \n",
    "    av_l2_loss_mri = np.average(l2_Loss_mri)\n",
    "    ep_l2_loss_mri.append(av_l2_loss_mri)\n",
    "    \n",
    "    if(epoch == EPOCH -1):\n",
    "        #Save a checkpoint\n",
    "        torch.save(cnn, 'C:/Users/horan/Desktop/FuseVis/.ipynb_checkpoints/DenseFuse/checkpoint.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.asarray(ep_ssim_loss_mri)\n",
    "l2 = np.asarray(ep_l2_loss_mri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('C:/Users/horan/Desktop/FuseVis/Loss curves/DenseFuse/H5 Files/Loss_L2.h5', 'w')\n",
    "h5f.create_dataset('L2_dataset', data=l2)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l1)\n",
    "plt.savefig('C:/Users/horan/Desktop/FuseVis/Loss curves/DenseFuse/Loss curves/SSIM_loss.png', bbox_inches = 'tight',pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test input MRI dataset\n",
    "filenames = os.listdir('C:/Users/horan/Desktop/FuseVis/MRI/')\n",
    "dataset = os.path.join(os.getcwd(), 'C:/Users/horan/Desktop/FuseVis/MRI/')\n",
    "data = glob.glob(os.path.join(dataset, \"*.gif\"))\n",
    "data = natsort.natsorted(data,reverse=False)\n",
    "test_mri = np.zeros((len(data), image_width,image_length))\n",
    "for i in range(len(data)):\n",
    "    test_mri[i,:,:] =(imageio.imread(data[i]))\n",
    "    test_mri[i,:,:] =(test_mri[i,:,:] - np.min(test_mri[i,:,:])) / (np.max(test_mri[i,:,:]) - np.min(test_mri[i,:,:]))\n",
    "    test_mri[i,:,:] = np.float32(test_mri[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand dimension to add the channel\n",
    "test_mri = np.expand_dims(test_mri,axis=1)\n",
    "#verify the shape matches the pytorch standard\n",
    "test_mri.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify the test mri image\n",
    "#test_mri = test_mri[0,:,:,:]\n",
    "#test_mri = np.expand_dims(test_mri,axis=0)\n",
    "plt.imshow(test_mri[0,0,:,:],'gray')\n",
    "#plt.savefig('MRI.png', bbox_inches = 'tight',pad_inches = 0,dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the MRI Testing data to pytorch tensor\n",
    "test_mri_tensor = torch.from_numpy(test_mri).float()\n",
    "test_mri_tensor = test_mri_tensor.to(device)\n",
    "print(test_mri_tensor.shape)\n",
    "test_mri_tensor.requires_grad =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test input PET dataset\n",
    "filenames = os.listdir('C:/Users/horan/Desktop/FuseVis/PET/')\n",
    "dataset = os.path.join(os.getcwd(), 'C:/Users/horan/Desktop/FuseVis/PET/')\n",
    "data = glob.glob(os.path.join(dataset, \"*.png\"))\n",
    "data = natsort.natsorted(data,reverse=False)\n",
    "test_pet = np.zeros((len(data), image_width,image_length))\n",
    "for i in range(len(data)):\n",
    "    test_pet[i,:,:] =(imageio.imread(data[i]))\n",
    "    test_pet[i,:,:] =(test_pet[i,:,:] - np.min(test_pet[i,:,:])) / (np.max(test_pet[i,:,:]) - np.min(test_pet[i,:,:]))\n",
    "    test_pet[i,:,:] = np.float32(test_pet[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand dimension to add the channel\n",
    "test_pet = np.expand_dims(test_pet,axis=1)\n",
    "#verify the shape matches the pytorch standard\n",
    "test_pet.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_pet[0,0,:,:],'gray')\n",
    "#plt.savefig('PET.png', bbox_inches = 'tight',pad_inches = 0,dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the MRI Testing data to pytorch tensor\n",
    "test_pet_tensor = torch.from_numpy(test_pet).float()\n",
    "test_pet_tensor = test_pet_tensor.to(device)\n",
    "print(test_pet_tensor.shape)\n",
    "test_pet_tensor.requires_grad =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn =torch.load('C:/Users/horan/Desktop/FuseVis/.ipynb_checkpoints/DenseFuse/checkpoint.pth')\n",
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_fusion_image(model, strategy_type, img1, img2):\n",
    "    # encoder\n",
    "    # test = torch.unsqueeze(img_ir[:, i, :, :], 1)\n",
    "    en_r = model.encoder(img1)\n",
    "    # vision_features(en_r, 'ir')\n",
    "    en_v = model.encoder(img2)\n",
    "    # vision_features(en_v, 'vi')\n",
    "    # fusion\n",
    "    f = model.fusion(en_r, en_v, strategy_type=strategy_type)\n",
    "    # f = en_v\n",
    "    # decoder\n",
    "    img_fusion = model.decoder(f)\n",
    "    return img_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted the fused image\n",
    "fused = _generate_fusion_image(cnn, 'addition', test_mri_tensor.to(device), test_pet_tensor.to(device))\n",
    "fused_numpy = fused.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify the output image\n",
    "plt.imshow(fused_numpy[0,0,:,:],'gray')\n",
    "#plt.savefig('Fused.png', bbox_inches = 'tight',pad_inches = 0,dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imageio.imwrite('C:/Users/horan/Desktop/FuseVis/Fused/DenseFuse/Fused.png',np.uint8(cv2.normalize(fused_numpy[0,0,:,:], None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the guidance image for MRI and PET wrt to the fused image\n",
    "time1 = time.time()\n",
    "count = 0 \n",
    "guide_fuse_mri = np.zeros((256,256),dtype=float)\n",
    "guide_fuse_pet = np.zeros((256,256),dtype=float)\n",
    "\n",
    "for y_coord in range(0,256):\n",
    "    for x_coord in range(0,256):\n",
    "        jacob_fuse_mri = torch.autograd.grad(fused[0,0,y_coord,x_coord], test_mri_tensor, retain_graph=True, create_graph=True)[0]\n",
    "        jacob_numpy_mri = np.squeeze(jacob_fuse_mri.data.cpu().numpy())  \n",
    "        guide_fuse_mri[y_coord,x_coord] = jacob_numpy_mri[y_coord,x_coord]\n",
    "        jacob_fuse_pet = torch.autograd.grad(fused[0,0,y_coord,x_coord], test_pet_tensor, retain_graph=True, create_graph=True)[0]\n",
    "        jacob_numpy_pet = np.squeeze(jacob_fuse_pet.data.cpu().numpy())  \n",
    "        guide_fuse_pet[y_coord,x_coord] = jacob_numpy_pet[y_coord,x_coord]\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print('Count is %d' %count)\n",
    "time2 = time.time()\n",
    "print('Time taken to compute is %d seconds' %(time2-time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h5f = h5py.File('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/H5 Files/Jacobian_MRI.h5', 'w')\n",
    "#h5f.create_dataset('Jacob_MRI_dataset', data=guide_fuse_mri)\n",
    "#h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(guide_fuse_mri,cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Guide_Fused_MRI_DPI.png', bbox_inches = 'tight',pad_inches = 0,dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h5f = h5py.File('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/H5 Files/Jacobian_PET.h5', 'w')\n",
    "#h5f.create_dataset('Jacob_PET_dataset', data=guide_fuse_pet)\n",
    "#h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(guide_fuse_pet,cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Guide_Fused_PET_DPI.png', bbox_inches = 'tight',pad_inches = 0,dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define overlay images\n",
    "fused_RGB = np.zeros((256,256,3),dtype=float)\n",
    "mri_RGB   = np.zeros((256,256,3),dtype=float)\n",
    "pet_RGB   = np.zeros((256,256,3),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_RGB[:,:,0]  = guide_fuse_mri \n",
    "fused_RGB[:,:,1]  = guide_fuse_pet \n",
    "fused_RGB[:,:,2]  = fused_numpy[0,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fused_RGB)\n",
    "plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Fused_RGB_DPI.png', bbox_inches = 'tight',pad_inches = 0,dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_RGB[:,:,0]  = guide_fuse_mri\n",
    "mri_RGB[:,:,1]  = guide_fuse_pet \n",
    "mri_RGB[:,:,2]  = test_mri[0,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mri_RGB)\n",
    "plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/MRI_RGB_DPI.png', bbox_inches = 'tight',pad_inches = 0,dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_RGB[:,:,0]  = guide_fuse_mri\n",
    "pet_RGB[:,:,1]  = guide_fuse_pet \n",
    "pet_RGB[:,:,2]  = test_pet[0,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pet_RGB)\n",
    "plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/PET_RGB_DPI.png', bbox_inches = 'tight',pad_inches = 0,dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the window\n",
    "root = Tk()  \n",
    "root.title('Visualisation of fusion networks')\n",
    "root.configure(background='white')\n",
    "\n",
    "\n",
    "#Label the images\n",
    "#fontStyle = tkFont.Font(family=\"Lucida Grande\", size=15)\n",
    "#w1 = tk.Label(root, bg='white', font=fontStyle, text=\"Fused Image\")\n",
    "#w1.grid(row=0, column=1)\n",
    "#w1.pack()\n",
    "\n",
    "#define the frame\n",
    "canvasframe = Frame(root)  # define Input and output frame\n",
    "buttonframe = Frame(root)  # define button frame\n",
    "canvasframe.pack()  # pack the Input and Output frame\n",
    "buttonframe.pack()  # pack the button frame\n",
    "\n",
    "\n",
    "#define the canvas\n",
    "canvas = Canvas(canvasframe, width=1800, height=920, bg = 'white')\n",
    "canvas.grid(row=0, column=0)\n",
    "\n",
    "#Insert fused image to the canvas\n",
    "img_fused = ImageTk.PhotoImage(file =\"C:/Users/horan/Desktop/FuseVis/Fused/DenseFuse/Fused.png\") # load the image\n",
    "canvas.create_image(0, 0, image=img_fused, anchor=NW)\n",
    "\n",
    "#Insert MRI image to the canvas\n",
    "img_mri = ImageTk.PhotoImage(file =\"C:/Users/horan/Desktop/FuseVis/MRI/MRI.gif\") # load the image\n",
    "canvas.create_image(600, 0, image=img_mri, anchor=NW)\n",
    "\n",
    "#Insert PET image to the canvas\n",
    "img_pet = ImageTk.PhotoImage(file =\"C:/Users/horan/Desktop/FuseVis/PET/3.png\") # load the image\n",
    "canvas.create_image(1200, 0, image=img_pet, anchor=NW)\n",
    "\n",
    "def start_mouseover():  # function called when user clicks the button \n",
    "    # link the function to the left-mouse-click event\n",
    "    canvas.bind(\"<B1-Motion>\", Coordinates)\n",
    "\n",
    "def Coordinates(event): # function called when left-mouse-button is clicked with a mouseover\n",
    "    x_coord = event.x  # save x and y coordinates selected by the user   \n",
    "    y_coord = event.y\n",
    "    print('mouse position is at' + '(' + str(y_coord) + ',' + str(x_coord) + ')', end='\\r')\n",
    "    #display the output MRI Jacobian image\n",
    "    #img_MR_out = ImageTk.PhotoImage(file ='C:/Users/cgvadmin/Desktop/FuseVis/Fused_MRI/im_' + str(y_coord) + '_' + str(x_coord) + '.png') # load the image\n",
    "    jacobian_fuse_mri = torch.autograd.grad(fused[0,0,y_coord,x_coord], test_mri_tensor, retain_graph=True, create_graph=True)[0]\n",
    "    jacobian_fuse_pet = torch.autograd.grad(fused[0,0,y_coord,x_coord], test_pet_tensor, retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "    jacob_val_mri = np.squeeze(jacobian_fuse_mri.data.cpu().numpy())    \n",
    "    jacob_val_pet = np.squeeze(jacobian_fuse_pet.data.cpu().numpy())\n",
    "    \n",
    "    x_mri = np.asarray(np.where(np.any(jacob_val_mri, axis = 0)))\n",
    "    y_mri = np.asarray(np.where(np.any(jacob_val_mri, axis = 1)))\n",
    "    minx_mri, maxx_mri, miny_mri, maxy_mri = np.min(x_mri), np.max(x_mri), np.min(y_mri), np.max(y_mri)  #return min and max coordinates\n",
    "    zoom_im_mri = jacob_val_mri[miny_mri:maxy_mri,minx_mri:maxx_mri] \n",
    "    \n",
    "    x_pet = np.asarray(np.where(np.any(jacob_val_pet, axis = 0)))\n",
    "    y_pet = np.asarray(np.where(np.any(jacob_val_pet, axis = 1)))\n",
    "    minx_pet, maxx_pet, miny_pet, maxy_pet = np.min(x_pet), np.max(x_pet), np.min(y_pet), np.max(y_pet)  #return min and max coordinates\n",
    "    zoom_im_pet = jacob_val_pet[miny_pet:maxy_pet,minx_pet:maxx_pet] \n",
    "    \n",
    "    plt.imshow(fused_numpy[0,0,miny_mri:maxy_mri,minx_mri:maxx_mri], cmap = 'gray', aspect ='equal')\n",
    "    plt.title('Zoom Fused')\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Zoom_Fused.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out11 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Zoom_Fused.png')\n",
    "    canvas.create_image(300,5,image=im_out11,anchor=NW)\n",
    "    canvas.image11 = im_out11\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.imshow(test_mri[0,0,miny_mri:maxy_mri,minx_mri:maxx_mri], cmap = 'gray', aspect ='equal')\n",
    "    plt.title('Zoom MRI')\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Zoom_MRI.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out12 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Zoom_MRI.png')\n",
    "    canvas.create_image(900,5,image=im_out12,anchor=NW)\n",
    "    canvas.image12 = im_out12\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.imshow(test_pet[0,0,miny_mri:maxy_mri,minx_mri:maxx_mri], cmap = 'gray', aspect ='equal')\n",
    "    plt.title('Zoom PET')\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Zoom_PET.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out13 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Zoom_PET.png')\n",
    "    canvas.create_image(1500,5,image=im_out13,anchor=NW)\n",
    "    canvas.image13 = im_out13\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.imshow(jacob_val_mri,cmap='viridis', aspect ='equal')\n",
    "    plt.title('Jacobian (Fused wrt MRI)')\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Jacob_Fused_MRI.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out1 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Jacob_Fused_MRI.png')\n",
    "    canvas.create_image(40,320,image=im_out1,anchor=NW)\n",
    "    canvas.image1 = im_out1\n",
    "    #plt.tight_layout()\n",
    "    \n",
    "    #f.add_subplot(1,5,2)\n",
    "    plt.imshow(zoom_im_mri,cmap='viridis',aspect ='equal')\n",
    "    plt.title('Zoom Jacobian (Fused wrt MRI)')\n",
    "    plt.colorbar()\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Zoom_Jacob_MRI.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out2 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Zoom_Jacob_MRI.png')\n",
    "    canvas.create_image(380,320,image=im_out2,anchor=NW)\n",
    "    canvas.image2 = im_out2\n",
    "    \n",
    "    #f.add_subplot(1,5,3)\n",
    "    plt.xlim(0,0.7)\n",
    "    plt.ylim(0,0.7)\n",
    "    plt.plot(jacob_val_mri[y_coord,x_coord],jacob_val_pet[y_coord,x_coord],'-ro')\n",
    "    plt.xlabel('MRI pixel score (Fused wrt MRI)')\n",
    "    plt.ylabel('PET pixel score (Fused wrt PET)')\n",
    "    plt.title('Mouse position at: (' + str(y_coord) + ',' + str(x_coord) + ')')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.draw()\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Pixel_intensities.png', bbox_inches = 'tight',pad_inches = 0.1)\n",
    "    plt.close()\n",
    "    im_out3 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Pixel_intensities.png')\n",
    "    canvas.create_image(750,320,image=im_out3,anchor=NW)\n",
    "    canvas.image3 = im_out3\n",
    "    \n",
    "    #f.add_subplot(1,5,4)\n",
    "    plt.imshow(jacob_val_pet,cmap='viridis',aspect ='equal')\n",
    "    plt.title('Jacobian (Fused wrt PET)')\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Jacob_Fused_PET.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out4 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Jacob_Fused_PET.png')\n",
    "    canvas.create_image(1110,320,image=im_out4,anchor=NW)\n",
    "    canvas.image4 = im_out4\n",
    "    #plt.tight_layout()\n",
    "    \n",
    "    #f.add_subplot(1,5,5)\n",
    "    plt.imshow(zoom_im_pet,cmap='viridis',aspect ='equal')\n",
    "    plt.title('Zoomed Jacobian (Fused wrt PET)')\n",
    "    plt.colorbar()\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Zoom_Jacob_PET.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out5 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Zoom_Jacob_PET.png')\n",
    "    canvas.create_image(1450,320,image=im_out5,anchor=NW)\n",
    "    canvas.image5 = im_out5\n",
    "\n",
    "    plt.imshow(guide_fuse_mri,cmap='viridis')\n",
    "    plt.title('Guidance (Fused wrt MRI)')\n",
    "    plt.colorbar()\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Guide_Fused_MRI.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out6 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Guide_Fused_MRI.png')\n",
    "    canvas.create_image(100,650,image=im_out6,anchor=NW)\n",
    "    canvas.image6 = im_out6\n",
    "    \n",
    "    plt.imshow(fused_RGB)\n",
    "    plt.title('R=I1 G=I2 B=Fused')\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Fused_RGB.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out8 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Fused_RGB.png')\n",
    "    canvas.create_image(420,650,image=im_out8,anchor=NW)\n",
    "    canvas.image8 = im_out8\n",
    "    \n",
    "    plt.imshow(mri_RGB)\n",
    "    plt.title('R=I1 G=I2 B=MRI')\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/MRI_RGB.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out9 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/MRI_RGB.png')\n",
    "    canvas.create_image(740,650,image=im_out9,anchor=NW)\n",
    "    canvas.image9 = im_out9\n",
    "    \n",
    "    plt.imshow(pet_RGB)\n",
    "    plt.title('R=I1 G=I2 B=PET')\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/PET_RGB.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out10 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/PET_RGB.png')\n",
    "    canvas.create_image(1100,650,image=im_out10,anchor=NW)\n",
    "    canvas.image10 = im_out10\n",
    "    \n",
    "    plt.imshow(guide_fuse_pet,cmap='viridis')\n",
    "    plt.title('Guidance (Fused wrt PET)')\n",
    "    plt.colorbar()\n",
    "    plt.savefig('C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Guide_Fused_PET.png', bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.close()\n",
    "    im_out7 = ImageTk.PhotoImage(file ='C:/Users/horan/Desktop/FuseVis/Guidance images/DenseFuse/Resultant images/Guide_Fused_PET.png')\n",
    "    canvas.create_image(1400,650,image=im_out7,anchor=NW)\n",
    "    canvas.image7 = im_out7\n",
    "    \n",
    "    radius = 5\n",
    "    i = canvas.create_oval(x_coord-radius, y_coord-radius, x_coord+radius, y_coord+radius, fill = 'red')\n",
    "    canvas.after(20,canvas.delete,i)\n",
    "\n",
    "# insert button to the middleframe and link it to \"Start Mouseover\"\n",
    "button_start_mouseover = Button(buttonframe, text=\"Start Mouseover\",command=start_mouseover)\n",
    "button_start_mouseover.grid(row=1, column=0, pady=0)\n",
    "\n",
    "\n",
    "root.mainloop()  #keep the GUI open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
